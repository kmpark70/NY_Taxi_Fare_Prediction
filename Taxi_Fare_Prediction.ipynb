{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Yellow Cab Fare Prediction\n",
    "\n",
    "### Dataset link: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page \n",
    "\n",
    "We selected the July data for Yellow Cab and convert parquet file to csv file for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark instance and pre-allocate Max memory to prevent errors.\n",
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-duration-prediction-2\")\\\n",
    "            .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "            .config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# \"Cast the \"total_amount column, trip_distance, passenger_count\" to an integer type \n",
    "# since the original type was string and we need to transform to double for concatenation later\n",
    "trips_df = trips_df.withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\"))\n",
    "trips_df = trips_df.withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))\n",
    "trips_df = trips_df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the Schema\n",
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformed for use in SQL: \"CAST the # total_amount column to an integer data type.\n",
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULocationID - pick up location ID\n",
    "# DOLocationID - drop off location ID\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    (SELECT\n",
    "        *,\n",
    "        TO_DATE(t.tpep_pickup_datetime) AS pickup_date\n",
    "    FROM\n",
    "        trips t)\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND pickup_date >= '2023-01-01'\n",
    "    AND pickup_date < '2023-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|            1.0|               161|                141|         0.97|          0|     Sunday|        14.3|\n",
      "|            1.0|                43|                237|          1.1|          0|     Sunday|        16.9|\n",
      "|            1.0|                48|                238|         2.51|          0|     Sunday|        34.9|\n",
      "|            0.0|               138|                  7|          1.9|          0|     Sunday|       20.85|\n",
      "|            1.0|               107|                 79|         1.43|          0|     Sunday|       19.68|\n",
      "|            1.0|               161|                137|         1.84|          0|     Sunday|        27.8|\n",
      "|            1.0|               239|                143|         1.66|          0|     Sunday|       20.52|\n",
      "|            1.0|               142|                200|         11.7|          0|     Sunday|       64.44|\n",
      "|            1.0|               164|                236|         2.95|          0|     Sunday|       28.38|\n",
      "|            1.0|               141|                107|         3.01|          0|     Sunday|        19.9|\n",
      "|            1.0|               234|                 68|          1.8|          0|     Sunday|       19.68|\n",
      "|            1.0|               164|                143|         3.23|          0|     Sunday|       37.32|\n",
      "|            2.0|               138|                 33|        11.43|          0|     Sunday|       66.31|\n",
      "|            1.0|                33|                 61|         2.95|          0|     Sunday|       24.24|\n",
      "|            1.0|                79|                186|         1.52|          0|     Sunday|       16.25|\n",
      "|            1.0|                90|                 48|         2.23|          0|     Sunday|       29.76|\n",
      "|            1.0|               113|                255|          4.5|          0|     Sunday|        29.5|\n",
      "|            3.0|               237|                239|          1.2|          0|     Sunday|        13.6|\n",
      "|            2.0|               143|                229|          2.5|          0|     Sunday|        20.6|\n",
      "|            1.0|               137|                 79|          1.4|          0|     Sunday|       17.15|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train set and test set\n",
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before reading a data, transform the type of data\n",
    "data_dir = \"/Users/kyungminpark/Desktop/Fall2023/CS4641/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 08:33:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.shp\n",
      "23/11/07 08:33:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.dbf\n",
      "23/11/07 08:33:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.shx\n",
      "23/11/07 08:33:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header:  DATUM[\"WGS84\",  6378137.0,  298.257223563]],  UNIT[\"degree\",  0.017453292519943295], \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found:  DATUM[\"WGS84\"\n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.prj\n",
      "23/11/07 08:33:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.shp\n",
      "23/11/07 08:33:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.dbf\n",
      "23/11/07 08:33:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , , \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found: \n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.shx\n",
      "23/11/07 08:33:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header:  DATUM[\"WGS84\",  6378137.0,  298.257223563]],  UNIT[\"degree\",  0.017453292519943295], \n",
      " Schema: tpep_pickup_datetime, passenger_count, trip_distance, PULocationID, DOLocationID, total_amount\n",
      "Expected: tpep_pickup_datetime but found:  DATUM[\"WGS84\"\n",
      "CSV file: file:///Users/kyungminpark/Desktop/Fall2023/CS4641/data/trips/taxi_zones.prj\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the schema, read the Parquet files, now we can use train_df and test_df anytime because we saved.\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: string (nullable = true)\n",
      " |-- dropoff_location_id: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical Feature PreProcessing Steps\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "cat_feats = [ ##categorical features\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "## Data flows through the pipeline stages (the pipeline consists of multiple stages, and each stage is executed sequentially)\n",
    "stages = []\n",
    "\n",
    "for c in cat_feats:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_486108410052,\n",
       " OneHotEncoder_21e3f664c40f,\n",
       " StringIndexer_94e6e8c43ee6,\n",
       " OneHotEncoder_281d6a9bad7b,\n",
       " StringIndexer_203c72cfb6b2,\n",
       " OneHotEncoder_801b9a77e3b9]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical Feature PreProcessing Steps\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"pickup_time\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vecotr\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate Categoriacal Feature and Numerical Feature\n",
    "\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "\n",
    "# Create the pipeline with the defined transformation stages\n",
    "pipeline = Pipeline(stages=transform_stages)\n",
    "## Fit the pipeline on the training data frame to create a fitted transformer\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: string (nullable = true)\n",
      " |-- dropoff_location_id: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- pickup_time_vecotr: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- passenger_count_vecotr: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vecotr: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used one of the \"Supervised Learning \" which is LinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\", ##총 택시비\n",
    "    featuresCol=\"feature_vector\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 08:33:39 WARN Instrumentation: [e7e7bcf6] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/11/07 08:33:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/07 08:33:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/11/07 08:33:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/11/07 08:33:42 WARN Instrumentation: [e7e7bcf6] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the test data frame\n",
    "vtest_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(vtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: double, pickup_location_id: string, dropoff_location_id: string, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, pickup_time_vecotr: vector, pickup_time_scaled: vector, passenger_count_vecotr: vector, passenger_count_scaled: vector, trip_distance_vecotr: vector, trip_distance_scaled: vector, feature_vector: vector, prediction: double]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          0.3|    Tuesday|        12.6|14.740826847804103|\n",
      "|          1.5|   Saturday|        14.0|17.954214720138758|\n",
      "|          1.9|   Thursday|       21.35|20.182057693408087|\n",
      "|          2.3|   Thursday|       29.85|23.614430372250908|\n",
      "|         16.7|     Friday|       90.55| 87.12648919285517|\n",
      "|          0.9|    Tuesday|        17.0|15.915257217012602|\n",
      "|          1.9|     Sunday|        22.4|19.520983350048976|\n",
      "|          2.6|  Wednesday|        27.9|24.495638038688984|\n",
      "|          3.3|    Tuesday|        47.0|27.323111652093385|\n",
      "|          3.3|     Monday|        29.7|26.673399221992423|\n",
      "|          2.3|  Wednesday|        33.2|23.425178359618254|\n",
      "|          0.6|  Wednesday|        15.4|16.090424087444426|\n",
      "|          0.8|  Wednesday|       14.25| 17.24093745398843|\n",
      "|          0.9|   Thursday|       14.25| 17.02556585112176|\n",
      "|          0.9|   Thursday|       17.15| 18.23457969435355|\n",
      "|          1.3|  Wednesday|       19.65|19.964621577015333|\n",
      "|          0.5|    Tuesday|        11.9|15.909614174837383|\n",
      "|          0.5|     Sunday|        10.5|14.453638528429039|\n",
      "|          0.8|   Thursday|       15.95|17.569109383887557|\n",
      "|          0.6|     Friday|        11.5|15.041705366189337|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select([\"trip_distance\", \"day_of_week\", \"total_amount\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is indicating the Yellow Cab fare prediction(Yellow Cab 2023 July data) for total_amount. Pay close attention to the two columns, total_amount, and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.671272023422926"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105461273892894"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the above result, the accuracy of prediction of total_amount is around 91%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
